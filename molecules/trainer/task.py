# Copyright 2018 Google Inc. All Rights Reserved. Licensed under the Apache
# License, Version 2.0 (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

from __future__ import print_function

import argparse
import dill as pickle
import multiprocessing as mp
import os
import tempfile

import tensorflow as tf

from tensorflow_transform.saved import saved_transform_io


DEFAULT_TEMP_DIR = os.path.join(
    tempfile.gettempdir(), 'cloudml-samples', 'molecules')


def make_estimator(run_config):
  """Here we create an estimator.

  In this case we're using a canned estimator.
  """
  estimator = tf.estimator.DNNRegressor(
      feature_columns=[
          tf.feature_column.numeric_column('NormalizedC', dtype=tf.float32),
          tf.feature_column.numeric_column('NormalizedH', dtype=tf.float32),
          tf.feature_column.numeric_column('NormalizedO', dtype=tf.float32),
          tf.feature_column.numeric_column('NormalizedN', dtype=tf.float32),
      ],
      hidden_units=[128, 64, 32],
      dropout=0.2,
      config=run_config)
  return estimator


def _make_train_or_eval_input_fn(
    feature_spec, labels, file_pattern, batch_size, mode, shuffle=True):

  def input_fn():
    def decode(elem):
      model_features = tf.parse_single_example(elem, features=feature_spec)
      model_labels = tf.stack([model_features.pop(label) for label in labels])
      return model_features, model_labels

    # For more information, check:
    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md
    files = tf.data.Dataset.list_files(file_pattern)
    dataset = files.apply(tf.contrib.data.parallel_interleave(
        tf.data.TFRecordDataset, cycle_length=mp.cpu_count()))
    dataset = dataset.map(decode, num_parallel_calls=mp.cpu_count())
    dataset = dataset.take(-1)
    dataset = dataset.cache()
    if mode == tf.estimator.ModeKeys.TRAIN:
      dataset = dataset.repeat()
      if shuffle:
        dataset = dataset.shuffle(batch_size * 8)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(1)

    iterator = dataset.make_one_shot_iterator()
    batch_features, batch_labels = iterator.get_next()
    return batch_features, batch_labels

  return input_fn


def make_train_input_fn(
    feature_spec, labels, file_pattern, batch_size, shuffle=True):
  """Makes an input_fn for training."""
  return _make_train_or_eval_input_fn(
      feature_spec,
      labels,
      file_pattern,
      batch_size,
      tf.estimator.ModeKeys.TRAIN,
      shuffle)


def make_eval_input_fn(feature_spec, labels, file_pattern, batch_size):
  """Makes an input_fn for evaluation."""
  return _make_train_or_eval_input_fn(
      feature_spec,
      labels,
      file_pattern,
      batch_size,
      tf.estimator.ModeKeys.EVAL)


def make_serving_input_fn(input_feature_spec, labels, transform_fn_dir=None):
  """Makes an input_fn for serving prediction.

  This will use the inputs format produced by the preprocessing PTransform. This
  applies the transformations from the tf.Transform preprocessing_fn before
  serving it to the TensorFlow model.
  """
  def serving_input_fn():
    input_features = {}
    for feature_name in input_feature_spec:
      if feature_name in labels:
        continue
      dtype = input_feature_spec[feature_name].dtype
      input_features[feature_name] = tf.placeholder(
          dtype, shape=[None], name=feature_name)

    if transform_fn_dir:
      _, inputs = saved_transform_io.partially_apply_saved_transform(
          transform_fn_dir, input_features)
    else:
      inputs = input_features

    return tf.estimator.export.ServingInputReceiver(inputs, input_features)

  return serving_input_fn


def dump(obj, filename):
  with tf.gfile.Open(filename, 'wb') as f:
    pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)


def load(filename):
  with tf.gfile.Open(filename, 'rb') as f:
    return pickle.load(f)


def train_and_evaluate(
    preprocess_data,
    estimator,
    model_name='model',
    batch_size=64,
    train_max_steps=1000):
  """Trains and evaluates the estimator given.

  The input functions are generated by the preprocessing function.
  """

  # Make the input functions
  train_input_fn = make_train_input_fn(
      preprocess_data.feature_spec,
      preprocess_data.labels,
      preprocess_data.train_files_pattern,
      batch_size)

  eval_input_fn = make_eval_input_fn(
      preprocess_data.feature_spec,
      preprocess_data.labels,
      preprocess_data.eval_files_pattern,
      batch_size)

  serving_input_fn = make_serving_input_fn(
      preprocess_data.input_feature_spec,
      preprocess_data.labels,
      preprocess_data.transform_fn_dir)

  # Make the TrainSpec and EvalSpec
  train_spec = tf.estimator.TrainSpec(
      input_fn=train_input_fn,
      max_steps=train_max_steps)

  exporter = tf.estimator.FinalExporter(model_name, serving_input_fn)

  eval_spec = tf.estimator.EvalSpec(
      input_fn=eval_input_fn,
      steps=None,
      name='eval',
      exporters=[exporter])

  # Train and evaluate
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)


if __name__ == '__main__':
  """Main function called by Cloud ML Engine."""
  parser = argparse.ArgumentParser(
      formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  parser.add_argument('--preprocess-data',
                      type=str,
                      default=os.path.join(DEFAULT_TEMP_DIR, 'PreprocessData'),
                      help='Path to a pickled PreprocessData object.')
  parser.add_argument('--export-dir',
                      type=str,
                      default=os.path.join(DEFAULT_TEMP_DIR, 'model'),
                      help='Path to export the TensorFlow model to. '
                           'This can be a Google Cloud Storage path.')
  parser.add_argument('--model-name',
                      type=str,
                      default='molecules',
                      help='Model name for the Exporter, will be used in the '
                           'export path')
  parser.add_argument('--batch-size',
                      type=int,
                      default=64,
                      help='Batch size for training and evaluation.')
  parser.add_argument('--train-max-steps',
                      type=int,
                      default=1000,
                      help='Number of steps to train the model')
  args = parser.parse_args()

  preprocess_data = load(args.preprocess_data)

  run_config = tf.estimator.RunConfig()
  run_config = run_config.replace(model_dir=args.export_dir)
  estimator = make_estimator(run_config)

  train_and_evaluate(
      preprocess_data,
      estimator,
      model_name=args.model_name,
      train_max_steps=args.train_max_steps)
