{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom Python Code in CMLE Prediction: User Guide",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Mt1WyuxKZTUu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Custom Python Code in CMLE Prediction: User Guide\n",
        "\n",
        "\n",
        "## Background\n",
        "Cloud ML Engine Online Prediction now supports custom python code in two forms:\n",
        "\n",
        "\n",
        "1.   Custom transforms in scikit-learn pipelines\n",
        "2.   Custom prediction routine\n",
        "\n",
        "\n",
        "Use the custom code in scikit-learn pipelines option if: \n",
        "\n",
        "*  Your trained scikit-learn pipeline uses a custom transform.\n",
        "\n",
        "\n",
        "Use the custom prediction routine option if:\n",
        "* Youâ€™re not using the scikit-learn framework and require custom processing.\n",
        "* You need to perform pre/post processing outside of the scikit-learn pipeline.\n",
        "* You need to load in saved state outside of the exported scikit-learn pipeline.\n",
        "\n",
        "**Note**: You must be whitelisted to use the custom code feature. Please fill out [this google form](https://goo.gl/forms/WgFzm97AJEpXiBDv2) to get started."
      ]
    },
    {
      "metadata": {
        "id": "p5WRaa94iyV5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Before we start let's install `gcloud` tool so we can interact with `Google Cloud Machine Learning Engine` easier:\n"
      ]
    },
    {
      "metadata": {
        "id": "fWgXiFA-Qda2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "256a4d79-55d6-48fb-f2a8-33a2150cfd6f"
      },
      "cell_type": "code",
      "source": [
        "!pip install google-cloud\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/b1/7c54d1950e7808df06642274e677dbcedba57f75307adf2e5ad8d39e5e0e/google_cloud-0.34.0-py2.py3-none-any.whl\n",
            "Installing collected packages: google-cloud\n",
            "Successfully installed google-cloud-0.34.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nlAua4RRQoMf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's also define the project name, model name, and the `gcs` bucket name that we'll refer to later:"
      ]
    },
    {
      "metadata": {
        "id": "ZWQQVse1QhAu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BUCKET=\"demo\"\n",
        "MODEL_DIR=\"model_files\"\n",
        "PACKAGES_DIR=\"my_packages\"\n",
        "PROJECT_NAME = 'demo-project'\n",
        "MODEL_NAME = 'demo_model'\n",
        "VERSION_NAME = 'v1'\n",
        "RUNTIME_VERSION = \"1.8\"\n",
        "\n",
        "!gcloud config set project {PROJECT_NAME}\n",
        "!gcloud ml-engine models create {MODEL_NAME}\n",
        "!gcloud ml-engine models list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EKhw6eHLQcnX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Option 1: Custom Code in scikit-learn Pipelines\n",
        "\n",
        "If you need to apply any custom data transformation that cannot be done via out of the box scikit-learn algorithms (such as [pre-processing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) or [feature_selection](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) modules), you can create your own python function and use it in your `Pipeline` using the `FunctionTransformer` wrapper. In order to have access to your custom function during prediction, you need to package it and upload it along with your model when you create a version. \n",
        "\n",
        "Here we give an example of how you can perform this. Assume we start with a basic iris training code similar to below:"
      ]
    },
    {
      "metadata": {
        "id": "w89WR2_oagGN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Load the digits dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Set up a pipeline with a feature selection preprocessor that\n",
        "# selects the top 2 features to use.\n",
        "# The pipeline then uses a RandomForestClassifier to train the\n",
        "# model.\n",
        "pipeline = Pipeline([\n",
        "      ('feature_selection', SelectKBest(chi2, k=2)),\n",
        "      ('classification', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "pipeline.fit(iris.data, iris.target)\n",
        "\n",
        "# Export the classifier to a file\n",
        "joblib.dump(pipeline, 'model.joblib')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "52mmf-h9ajhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you want to add a preprocessing step to append the sum of all features as a new feature. You can write a simple method that performs this feature manipulation in a file called my_module.py:\n"
      ]
    },
    {
      "metadata": {
        "id": "XxAtuxc5NbGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c1dbd5d-c6b3-4a74-d4c9-e19155d58397"
      },
      "cell_type": "code",
      "source": [
        "%%writefile my_module.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def add_sum(X):\n",
        "\n",
        "  # Append the sum of each row as a new feature.\n",
        "  sums = X.sum(1)[...,None]\n",
        "  new_features = np.append(X, sums, 1)\n",
        "  return new_features\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing my_module.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tbUyHu90Nfpx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can now modify your training code, to use this function in your scikit-learn pipeline:"
      ]
    },
    {
      "metadata": {
        "id": "lyF5WReXNiwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0668c0ae-4a48-49b9-bab7-0343a0d5c06f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.externals import joblib\n",
        "from my_module import add_sum\n",
        "\n",
        "# Load the digits dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Set up a pipeline with a feature selection preprocessor that\n",
        "# selects the top 2 features to use.\n",
        "# The pipeline then uses a RandomForestClassifier to train the\n",
        "# model.\n",
        "pipeline = Pipeline([\n",
        "      ('add_sum_step', FunctionTransformer(add_sum)),\n",
        "      ('feature_selection', SelectKBest(chi2, k=2)),\n",
        "      ('classification', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "pipeline.fit(iris.data, iris.target)\n",
        "\n",
        "# Export the classifier to a file\n",
        "joblib.dump(pipeline, 'model.joblib')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "dxlI1_RtNllJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The rest of training and exporting the model remains the same. So we can go ahead and upload this exported model to `MODEL_DIR`:\n"
      ]
    },
    {
      "metadata": {
        "id": "8M1UYRECR5K4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e0a39db7-de39-4cb7-8928-a2a51647d391"
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp ./model.joblib gs://{BUCKET}/{MODEL_DIR}/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./model.joblib [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 21.2 KiB/ 21.2 KiB]                                                \n",
            "Operation completed over 1 objects/21.2 KiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kcr_F1C0R6EV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order for your model to have access to `my_module` at prediction time, you need to package and upload it along with your exported model. To do so, first you need to create a pip-installable package. One way to do so is to add a setup.py file in the same directory similar to this example:\n"
      ]
    },
    {
      "metadata": {
        "id": "c0veWykrNmVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f18cbde-4abe-4a4e-934f-39eb7c815665"
      },
      "cell_type": "code",
      "source": [
        "%%writefile setup.py\n",
        "from setuptools import setup\n",
        "setup(name=\"my_package\",\n",
        "      version=\"0.1\",\n",
        "      scripts=[\"my_module.py\"]\n",
        "      )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "itf9993fPCvy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then you can create a package by running:"
      ]
    },
    {
      "metadata": {
        "id": "GnF-geCZNtBR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "b61555a4-4f29-46d4-fe81-4c3c439e4057"
      },
      "cell_type": "code",
      "source": [
        "!python setup.py sdist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running sdist\r\n",
            "running egg_info\r\n",
            "creating my_package.egg-info\r\n",
            "writing my_package.egg-info/PKG-INFO\r\n",
            "writing top-level names to my_package.egg-info/top_level.txt\r\n",
            "writing dependency_links to my_package.egg-info/dependency_links.txt\r\n",
            "writing manifest file 'my_package.egg-info/SOURCES.txt'\r\n",
            "reading manifest file 'my_package.egg-info/SOURCES.txt'\r\n",
            "writing manifest file 'my_package.egg-info/SOURCES.txt'\r\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\r\n",
            "\r\n",
            "running check\r\n",
            "warning: check: missing required meta-data: url\r\n",
            "\r\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\r\n",
            "\r\n",
            "creating my_package-0.1\r\n",
            "creating my_package-0.1/my_package.egg-info\r\n",
            "copying files to my_package-0.1...\r\n",
            "copying my_module.py -> my_package-0.1\r\n",
            "copying setup.py -> my_package-0.1\r\n",
            "copying my_package.egg-info/PKG-INFO -> my_package-0.1/my_package.egg-info\r\n",
            "copying my_package.egg-info/SOURCES.txt -> my_package-0.1/my_package.egg-info\r\n",
            "copying my_package.egg-info/dependency_links.txt -> my_package-0.1/my_package.egg-info\r\n",
            "copying my_package.egg-info/top_level.txt -> my_package-0.1/my_package.egg-info\n",
            "Writing my_package-0.1/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'my_package-0.1' (and everything under it)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mSlC2fLbNwh4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will create a `.tar.gz` package under `/dist` directory. The name of the package will be `$name-$version.tar.gz` where `$name` and `$version` are the ones specified in the `setup.py`. \n",
        "\n",
        "Once you have successfully created the package, you can upload it to `GCS`:\n"
      ]
    },
    {
      "metadata": {
        "id": "2ItnplKxN66J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "efec3e7a-3d4c-4942-f352-45704235fd0c"
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp ./dist/my_package-0.1.tar.gz gs://{BUCKET}/{PACKAGES_DIR}/my_package-0.1.tar.gz\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./dist/my_package-0.1.tar.gz [Content-Type=application/x-tar]...\n",
            "\n",
            "Operation completed over 1 objects/705.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oau1IVgXi5hM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil acl -r ch -u AllUsers:O gs://{BUCKET}/{PACKAGES_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gotOaIcEjFh9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://{BUCKET}/{PACKAGES_DIR}/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMZneKw7Q7Qo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dcf0eac3-f507-4f51-9507-a4ec1d8d79b4"
      },
      "cell_type": "code",
      "source": [
        "!gcloud alpha ml-engine versions create {VERSION_NAME} --model {MODEL_NAME} \\\n",
        " --origin gs://{BUCKET}/{MODEL_DIR} \\\n",
        " --runtime-version {RUNTIME_VERSION} \\\n",
        " --framework SCIKIT_LEARN \\\n",
        " --package-uris gs://{BUCKET}/{PACKAGES_DIR}/my_package-0.1.tar.gz\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating version (this might take a few minutes)......done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NuG3ncEbTNft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once creating the version is finished (should take 1-2 minutes) you can send a prediction request to your model:"
      ]
    },
    {
      "metadata": {
        "id": "eTka-Rp7TY2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "565fc8d9-36c6-4860-a71a-333ab7ec7e4b"
      },
      "cell_type": "code",
      "source": [
        "%%writefile input.json\n",
        "[1, 2, 3, 4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing input.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H5MTBoqJTbtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "654bae1e-a0c7-49f6-c489-68522ac73a1e"
      },
      "cell_type": "code",
      "source": [
        "!gcloud alpha ml-engine predict --model {MODEL_NAME} --version {VERSION_NAME} --json-instances input.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eGBGwfwoT37s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Option 2: Custom Prediction Routine\n",
        "\n",
        "If you need to apply any custom data transformation before or after prediction is applied to the input, you can provide a custom python class along with your exported model. However, itâ€™s important to be aware of the dangers of training-serving skew which can happen by inadvertently applying different preprocessing functions at prediction time vs. training time, and thus introduce a bias. \n",
        "\n",
        "To avoid this problem, we strongly recommend placing all the preprocessing related code into one method so it can be re-used during both training and prediction.\n",
        "\n",
        "To do so, you can upload a custom class along with your exported model. This class should implement the `Model` interface shown below:"
      ]
    },
    {
      "metadata": {
        "id": "AUxSVg1SUKIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "  \"\"\"Interface for constructing custom models.\"\"\"\n",
        "\n",
        "  def predict(self, instances, **kwargs):\n",
        "    \"\"\"Performs custom prediction.\n",
        "\n",
        "    Instances are the decoded values from the request. Clients need not worry\n",
        "    about decoding json nor base64 decoding.\n",
        "\n",
        "    Args:\n",
        "      instances: list of instances, as described in the API.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "      A list of outputs containing the prediction results. This list must be \n",
        "      JSON serializable.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  @classmethod\n",
        "  def from_path(cls, model_dir):\n",
        "    \"\"\"Creates an instance of Model using the given path.\n",
        "\n",
        "    Loading of the model should be done in this method.\n",
        "\n",
        "    Args:\n",
        "      model_dir: The local directory that contains the exported model file \n",
        "          along with any additional files uploaded when creating the version \n",
        "          resource.\n",
        "\n",
        "    Returns:\n",
        "      An instance implementing this Model class.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ks3P9Q0pUPs0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your implementaion of `Model` class should have a `predict` method that accepts  a list of input `instances`, and `**kwargs` which will hold any additional parameter you need to pass to your function.\n",
        "\n",
        "> **Note**:  Everything included in the directory that's passsed as `origin` flag when creating a version would be available in the `model_dir` argument of `from_path` method.\n",
        "\n",
        "The following examples show how to use this feature in two different ways.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bDIDd1e5ULKr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example 1\n",
        "\n",
        "In this example we show how to call a custom `preprocess` function by customizing the `predict` method.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cqFgl0NPbb66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training the model\n",
        "\n",
        "Letâ€™s start with the following existing code for training which loads training data from a file and  performs the following preprocessing: \n",
        "\n",
        "* takes log of values in the second column \n",
        "* replaces `-inf` and `+inf` values by `0`:\n",
        "\n",
        "Finally it fits a classifier and exports the trained model to a file (`model.joblib`):"
      ]
    },
    {
      "metadata": {
        "id": "oTiiFW3mU3ZD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b9d86b7b-279e-4aaf-8321-0212cd3c4a5d"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Load the dataset\n",
        "iris = datasets.load_iris()\n",
        "data = pd.DataFrame(iris.data)\n",
        "# Preprocess the data\n",
        "data.iloc[:, 2] = np.log(data.iloc[:,2])\n",
        "data[~np.isfinite(data)] = 0 \n",
        "\n",
        "# Train a classifier\n",
        "classifier = svm.SVC()\n",
        "classifier.fit(data, iris.target)\n",
        "\n",
        "# Export the classifier to a file\n",
        "joblib.dump(classifier, 'model.joblib')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "6giBbiE6U8O7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now to make this script fit into the recommended approach above, we can make a few changes, so that the preprocessing happens in a separate class that can be shared at both training and prediction time: "
      ]
    },
    {
      "metadata": {
        "id": "3kCX8ywmU_Ks",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cfff30c0-b418-458f-b602-eac0a0acbc1b"
      },
      "cell_type": "code",
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "class MyProcessor(object): \n",
        "    \n",
        "\n",
        "  def preprocess(self, data):            \n",
        "    data.iloc[:, 2] = np.log(data.iloc[:,2])\n",
        "    data[~np.isfinite(data)] = 0\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing preprocess.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PnVd99VNVHvR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the training script should now reference the preprocessing module:"
      ]
    },
    {
      "metadata": {
        "id": "0CEje0m6VDzO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1cc8f566-7f0a-4547-a28c-435465570811"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from preprocess import MyProcessor\n",
        "\n",
        "# Load the dataset\n",
        "iris = datasets.load_iris()\n",
        "data = pd.DataFrame(iris.data)\n",
        "\n",
        "# Preprocess the data\n",
        "processor = MyProcessor()\n",
        "data = processor.preprocess(data)\n",
        "\n",
        "\n",
        "# Train a classifier\n",
        "classifier = svm.SVC()\n",
        "classifier.fit(data, iris.target)\n",
        "\n",
        "# Export the classifier to a file\n",
        "joblib.dump(classifier, 'model.joblib')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "cYmShdGjVZ8q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Defining the custom `Model` class\n",
        "\n",
        "In order to provide a custom prediction logic, we need  to implement the `Model` API shown above. Our example below shows how to load the model object (`model.joblib`) and perform predictions."
      ]
    },
    {
      "metadata": {
        "id": "Yb_jj09hVgVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90a7149c-c510-4f40-a30d-50922301bbd4"
      },
      "cell_type": "code",
      "source": [
        "%%writefile my_model.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.externals import joblib\n",
        "from preprocess import MyProcessor\n",
        "import pickle\n",
        "\n",
        "class ModelExample(object):\n",
        "\n",
        "  def __init__(self, model):\n",
        "    self._model = model\n",
        "    self._processor = MyProcessor() \n",
        "\n",
        "  def predict(self, instances, **kwargs):\n",
        "    data = pd.DataFrame(instances)\n",
        "    preprocessed_data = self._processor.preprocess(data)\n",
        "    return self._model.predict(preprocessed_data).tolist()\n",
        "\n",
        "  @classmethod\n",
        "  def from_path(cls, model_dir):\n",
        "    model = joblib.load(os.path.join(model_dir, 'model.joblib'))    \n",
        "    return cls(model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing my_model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cyivy024Vpcy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "####Packaging up files and uploading to GCS\n",
        "\n",
        "Just like in the normal workflow, we need to upload our model file, and any other data files we want to access from our custom model."
      ]
    },
    {
      "metadata": {
        "id": "gBA0554pWIJ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp model.joblib gs://{BUCKET}/{MODEL_DIR}/\n",
        "!gsutil ls gs://{BUCKET}/{MODEL_DIR}/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UG030k_CWjmO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we need to package the classes that are going to be needed at prediction time. We need to create a pip installable tar file with our `model.py` and `processor.py`:\n"
      ]
    },
    {
      "metadata": {
        "id": "SFSugwYUWtVA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1afdf65a-2198-4683-e308-377de175e78e"
      },
      "cell_type": "code",
      "source": [
        "%%writefile setup.py\n",
        "from setuptools import setup\n",
        "setup(name=\"my_package\",\n",
        "      version=\"0.1\",\n",
        "      scripts=[\"preprocess.py\", \"my_model.py\"]\n",
        "      )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uY71rNsIXJRC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then you can create a package by running:"
      ]
    },
    {
      "metadata": {
        "id": "mQhiOF-benZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "822da636-489a-4c9a-8b8b-26340d73e478"
      },
      "cell_type": "code",
      "source": [
        "!python setup.py sdist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running sdist\n",
            "running egg_info\n",
            "creating my_package.egg-info\n",
            "writing my_package.egg-info/PKG-INFO\n",
            "writing top-level names to my_package.egg-info/top_level.txt\n",
            "writing dependency_links to my_package.egg-info/dependency_links.txt\n",
            "writing manifest file 'my_package.egg-info/SOURCES.txt'\n",
            "reading manifest file 'my_package.egg-info/SOURCES.txt'\n",
            "writing manifest file 'my_package.egg-info/SOURCES.txt'\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "running check\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "creating my_package-0.1\n",
            "creating my_package-0.1/my_package.egg-info\n",
            "copying files to my_package-0.1...\n",
            "copying my_model.py -> my_package-0.1\n",
            "copying preprocess.py -> my_package-0.1\n",
            "copying setup.py -> my_package-0.1\n",
            "copying my_package.egg-info/PKG-INFO -> my_package-0.1/my_package.egg-info\n",
            "copying my_package.egg-info/SOURCES.txt -> my_package-0.1/my_package.egg-info\n",
            "copying my_package.egg-info/dependency_links.txt -> my_package-0.1/my_package.egg-info\n",
            "copying my_package.egg-info/top_level.txt -> my_package-0.1/my_package.egg-info\n",
            "Writing my_package-0.1/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'my_package-0.1' (and everything under it)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8D1F_yGenZp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will create a `.tar.gz` package under `/dist` directory. The name of the package will be `$name-$version.tar.gz` where `$name` and `$version` are the ones specified in the `setup.py`. \n",
        "\n",
        "Once you have successfully created the package, you can upload it to `GCS`:\n"
      ]
    },
    {
      "metadata": {
        "id": "pY8OJ-juenZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "287ad72d-f054-4853-fc96-d67eb94bf75d"
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp ./dist/my_package-0.1.tar.gz gs://{BUCKET}/{PACKAGES_DIR}/my_package-0.1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./dist/my_package-0.1.tar.gz [Content-Type=application/x-tar]...\n",
            "/ [1 files][   1023 B/   1023 B]                                                \n",
            "Operation completed over 1 objects/1023.0 B.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HJ6Bar9-fPw9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create a `Version`\n",
        "\n",
        "Once you have your custom package ready, you can specify this as an argument when creating a `version` resource. Note that you need to provide the path to your package (as `package-uris`) and also the class name that contains your custom `predict` method (as `model-class`). \n"
      ]
    },
    {
      "metadata": {
        "id": "7i_fJMT8T_cS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f7b9e27-c906-4cf1-892b-ec44fe90bb10"
      },
      "cell_type": "code",
      "source": [
        "!gcloud alpha ml-engine versions list --model {MODEL_NAME}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Listed 0 items.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b8zB7DCHUCcm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gcloud alpha ml-engine versions create {VERSION_NAME} --model {MODEL_NAME} \\\n",
        "--origin gs://{BUCKET}/{MODEL_DIR}/ \\\n",
        "--runtime-version {RUNTIME_VERSION} \\\n",
        "--package-uris gs://{BUCKET}/{PACKAGES_DIR}/my_package-0.1.tar.gz \\\n",
        "--model-class=my_model.ModelExample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4O576vzhgS2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once creating the version is finished (should take 1-2 minutes) you can send a prediction request to your model:"
      ]
    },
    {
      "metadata": {
        "id": "yvpIqifVhkS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb062944-fee1-432f-c215-106767001438"
      },
      "cell_type": "code",
      "source": [
        "%%writefile input.json\n",
        "[1, 2, 3, 4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing input.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S2AgWpC2hkTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dabca89e-7e37-40f7-aa15-09595104ea37"
      },
      "cell_type": "code",
      "source": [
        "!gcloud alpha ml-engine predict --model {MODEL_NAME} --version {VERSION_NAME} --json-instances input.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UXzIvkDBXz5-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example 2\n",
        "\n",
        "This example is very similar to the one above, with a slight difference that it also uses additional artifact that was produced during training. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mUBb19D2YGSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Training the model\n",
        "\n",
        "As in the example above, we start with the training code that performs the following preprocessing, with an addition that it also binarizes the first column based on the mean value of that column. This data is available at training time, but not at serving time. Therefore we need to store the `mean` value somewhere and use it in our custom predict funtion later.\n"
      ]
    },
    {
      "metadata": {
        "id": "-wesFJcaYGSM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f915c9e4-d465-42ab-a049-25c70485f5df"
      },
      "cell_type": "code",
      "source": [
        "%%writefile stateful_preprocess.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "class MyStatefulProcessor(object):\n",
        "  def __init__(self):\n",
        "    self._mean = None\n",
        "\n",
        "  def preprocess(self, data):        \n",
        "    if not self._mean: # during training\n",
        "      self._mean = np.mean(data, axis=0)[0]\n",
        "    data.iloc[:, 0] = (data.iloc[:,0] > self._mean) * 1\n",
        "    data.iloc[:, 2] = np.log(data.iloc[:,2])\n",
        "    data[~np.isfinite(data)] = 0\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing stateful_preprocess.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-gHW6j6oYGSR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And our training script:"
      ]
    },
    {
      "metadata": {
        "id": "LSjKWW0OYGSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebefd1ac-490d-4569-c875-3351f9bf1cdb"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from stateful_preprocess import MyStatefulProcessor\n",
        "\n",
        "# Load the dataset\n",
        "iris = datasets.load_iris()\n",
        "data = pd.DataFrame(iris.data)\n",
        "\n",
        "# Preprocess the data\n",
        "processor = MyStatefulProcessor()\n",
        "data = processor.preprocess(data)\n",
        "\n",
        "\n",
        "# Train a classifier\n",
        "classifier = svm.SVC()\n",
        "classifier.fit(data, iris.target)\n",
        "\n",
        "# Export the classifier to a file\n",
        "joblib.dump(classifier, 'model.joblib')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "OUjmgXL4YGSb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In addition to the `model.joblib` file, we also export our `processor` object since it holds the `mean` value observed during training. We can reference it later during prediction:"
      ]
    },
    {
      "metadata": {
        "id": "KunHVWyXYGSc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('./my_processor_state.pkl', 'wb') as f:\n",
        "  pickle.dump(processor, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3a2E-FMiYGSe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Defining the custom `Model` class\n",
        "\n",
        "As in example 1, we need  to implement the `Model` API for our custom prediction module. The difference here is that it also loads the custom pickled files (in this case `my_processor.pkl`) as well as the model object (`model.joblib`) when the model is being loaded. This happens because every object that is included in the `model_dir` will be available in the local file system in the `model_dir` path that gets passed to the `from_path` method."
      ]
    },
    {
      "metadata": {
        "id": "67lJqXA_YGSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d6ff42d6-b71b-4fc2-8842-62ae312c0d67"
      },
      "cell_type": "code",
      "source": [
        "%%writefile my_model.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.externals import joblib\n",
        "from stateful_preprocess import MyStatefulProcessor\n",
        "import pickle\n",
        "\n",
        "class ModelExample(object):\n",
        "\n",
        "  def __init__(self, model, processor):\n",
        "    self._model = model\n",
        "    self._processor = processor\n",
        "\n",
        "\n",
        "  def predict(self, instances, **kwargs):\n",
        "    data = pd.DataFrame(instances)\n",
        "    preprocessed_data = self._processor.preprocess(data)\n",
        "    return self._model.predict(preprocessed_data).tolist()\n",
        "\n",
        "  @classmethod\n",
        "  def from_path(cls, model_dir):\n",
        "    model = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
        "    with open(os.path.join(model_dir, 'my_processor_state.pkl'), 'rb') as f:\n",
        "      processor = pickle.load(f)\n",
        "    return cls(model, processor)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting my_model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tx8YlM4PYGSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "####Packaging up files and uploading to GCS\n",
        "\n",
        "To be able to access any additional artifacts during prediction time, we need to put them in the same `gcs` bucket as our model: \n"
      ]
    },
    {
      "metadata": {
        "id": "Yrov-MTGYGSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a16ede13-3751-4918-a9d2-55ee536a8eaa"
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp model.joblib gs://{BUCKET}/{MODEL_DIR}/\n",
        "!gsutil cp my_processor_state.pkl gs://{BUCKET}/{MODEL_DIR}/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://model.joblib [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  5.1 KiB/  5.1 KiB]                                                \n",
            "Operation completed over 1 objects/5.1 KiB.                                      \n",
            "Copying file://my_processor_state.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  272.0 B/  272.0 B]                                                \n",
            "Operation completed over 1 objects/272.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QbRL286cYGSs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The rest of the work will be the same as example 1: Let's create a pip installable tar file with our `model.py` and `processor.py`:\n"
      ]
    },
    {
      "metadata": {
        "id": "GMKbpXN9YGSt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0eadef3-d224-42b2-bf62-7d026662390f"
      },
      "cell_type": "code",
      "source": [
        "%%writefile setup.py\n",
        "from setuptools import setup\n",
        "setup(name=\"my_package\",\n",
        "      version=\"0.2\",\n",
        "      scripts=[\"stateful_preprocess.py\", \"my_model.py\"]\n",
        "      )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b-9xnJqjYGSw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we can create a package by running:"
      ]
    },
    {
      "metadata": {
        "id": "WzP_E8-CYGSx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "22ed1fdc-019a-4eb4-d93d-af2be093fe37"
      },
      "cell_type": "code",
      "source": [
        "!python setup.py sdist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running sdist\n",
            "running egg_info\n",
            "writing my_package.egg-info/PKG-INFO\n",
            "writing top-level names to my_package.egg-info/top_level.txt\n",
            "writing dependency_links to my_package.egg-info/dependency_links.txt\n",
            "reading manifest file 'my_package.egg-info/SOURCES.txt'\n",
            "writing manifest file 'my_package.egg-info/SOURCES.txt'\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "running check\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "creating my_package-0.2\n",
            "creating my_package-0.2/my_package.egg-info\n",
            "copying files to my_package-0.2...\n",
            "copying my_model.py -> my_package-0.2\n",
            "copying preprocess.py -> my_package-0.2\n",
            "copying setup.py -> my_package-0.2\n",
            "copying stateful_preprocess.py -> my_package-0.2\n",
            "copying my_package.egg-info/PKG-INFO -> my_package-0.2/my_package.egg-info\n",
            "copying my_package.egg-info/SOURCES.txt -> my_package-0.2/my_package.egg-info\n",
            "copying my_package.egg-info/dependency_links.txt -> my_package-0.2/my_package.egg-info\n",
            "copying my_package.egg-info/top_level.txt -> my_package-0.2/my_package.egg-info\n",
            "Writing my_package-0.2/setup.cfg\n",
            "Creating tar archive\n",
            "removing 'my_package-0.2' (and everything under it)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mU7uQKySYGS2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will create a `.tar.gz` package under `/dist` directory. The name of the package will be `$name-$version.tar.gz` where `$name` and `$version` are the ones specified in the `setup.py`. \n",
        "\n",
        "Once you have successfully created the package, you can upload it to `GCS`:\n"
      ]
    },
    {
      "metadata": {
        "id": "JnmSGNhYYGS4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "41ceb836-ce7e-446b-bcef-0acf256c5868"
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp ./dist/my_package-0.2.tar.gz gs://{BUCKET}/{PACKAGES_DIR}/my_package-0.2.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./dist/my_package-0.2.tar.gz [Content-Type=application/x-tar]...\n",
            "/ [1 files][  1.1 KiB/  1.1 KiB]                                                \n",
            "Operation completed over 1 objects/1.1 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FdGglxk9YGS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create a `Version`\n",
        "\n",
        "Once you have your custom package ready, you can specify this as an argument when creating a `version` resource. Note that you need to provide the path to your package (as `package-uris`) and also the class name that contains your custom `predict` method (as `model-class`). \n"
      ]
    },
    {
      "metadata": {
        "id": "7jd2H-TGYGS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "05fa6269-90bd-46da-dc79-2cd00c5ad335"
      },
      "cell_type": "code",
      "source": [
        "!gcloud alpha ml-engine versions list --model {MODEL_NAME}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME  DEPLOYMENT_URI                STATE\n",
            "v1    gs://nedam-iris/model_files/  READY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6qfgS4WCYGTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VERSION_NAME='v2'\n",
        "!gcloud alpha ml-engine versions create {VERSION_NAME} --model {MODEL_NAME} \\\n",
        "--origin gs://{BUCKET}/{MODEL_DIR}/ \\\n",
        "--runtime-version {RUNTIME_VERSION} \\\n",
        "--package-uris gs://{BUCKET}/{PACKAGES_DIR}/my_package-0.2.tar.gz \\\n",
        "--model-class=my_model.ModelExample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wq2Y4Y59YGTC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once creating the version is finished (should take 1-2 minutes) you can send a prediction request to your model:"
      ]
    },
    {
      "metadata": {
        "id": "TGQ5S9MQYGTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f4ebf4d-e166-464b-aafd-08fb08751191"
      },
      "cell_type": "code",
      "source": [
        "%%writefile input.json\n",
        "[1, 2, 3, 4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting input.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t-0kexL5YGTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee781012-3772-4d3f-f9ed-0ad1605b61ac"
      },
      "cell_type": "code",
      "source": [
        "!gcloud alpha ml-engine predict --model {MODEL_NAME} --version {VERSION_NAME} --json-instances input.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "adA6Mh4VJCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Questions? Feedback?\n",
        "\n",
        "Feel free to send us an email (cloudml-feedback@google.com) if you run into any issues or have any questions/feedback!\n"
      ]
    }
  ]
}