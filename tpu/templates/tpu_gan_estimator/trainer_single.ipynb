{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 5\n",
    "OUTPUT_DIM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fn(generator_inputs):\n",
    "    outputs = tf.layers.dense(generator_inputs, OUTPUT_DIM)\n",
    "    return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_fn(data, generator_inputs):\n",
    "    outputs = tf.layers.dense(data, 1)\n",
    "    return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # build model\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    generator_inputs = features\n",
    "    real_data = labels\n",
    "\n",
    "    gan_model = tf.contrib.gan.gan_model(generator_fn, discriminator_fn, real_data, generator_inputs)\n",
    "\n",
    "    predictions = gan_model.generated_data\n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # define loss\n",
    "        gan_loss = tf.contrib.gan.gan_loss(gan_model, add_summaries=False)\n",
    "        loss = gan_loss.generator_loss\n",
    "\n",
    "        # define train_op\n",
    "        gen_optimizer = tf.train.RMSPropOptimizer(learning_rate=0.05)\n",
    "        dis_optimizer = tf.train.RMSPropOptimizer(learning_rate=0.05)\n",
    "\n",
    "        # wrapper to make the optimizer work with TPUs\n",
    "        if params['use_tpu']:\n",
    "            gen_optimizer = tf.contrib.tpu.CrossShardOptimizer(gen_optimizer)\n",
    "            dis_optimizer = tf.contrib.tpu.CrossShardOptimizer(dis_optimizer)\n",
    "\n",
    "        gan_train_ops = tf.contrib.gan.gan_train_ops(gan_model, gan_loss, gen_optimizer, dis_optimizer)\n",
    "\n",
    "        while_loop = tf.contrib.tpu.while_loop if params['use_tpu'] else tf.while_loop\n",
    "\n",
    "        # train the discriminator 100 steps\n",
    "        inputs = [tf.constant(0), tf.constant(0.0)]\n",
    "        cond = lambda i, x: tf.less(i, 100)\n",
    "        def body(i, x):\n",
    "            return tf.add(i, 1), gan_train_ops.discriminator_train_op\n",
    "\n",
    "        dis_train_op = while_loop(cond, body, inputs)\n",
    "\n",
    "        # tf.contrib.gan's train op does not manage global steps in it\n",
    "        train_op = tf.group(\n",
    "            dis_train_op,\n",
    "            gan_train_ops.generator_train_op,\n",
    "            global_step.assign_add(1)\n",
    "        )\n",
    "\n",
    "    if params['use_tpu']:\n",
    "        # TPU version of EstimatorSpec\n",
    "        return tf.contrib.tpu.TPUEstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "        )\n",
    "    else:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(params={}):\n",
    "    # make some fake noise\n",
    "    data_size = 100\n",
    "    noise_tensor = tf.random_normal((data_size, INPUT_DIM))\n",
    "    real_data_tensor = tf.random_uniform((data_size, OUTPUT_DIM))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((noise_tensor, real_data_tensor))\n",
    "    dataset = dataset.repeat().shuffle(10)\n",
    "\n",
    "    # TPUEstimator passes params when calling input_fn\n",
    "    batch_size = params.get('train_batch_size', 16)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # TPUs need to know all dimensions when the graph is built\n",
    "    # Datasets know the batch size only when the graph is run\n",
    "    def set_shapes(features, labels):\n",
    "        features_shape = features.get_shape().merge_with([batch_size, None])\n",
    "        labels_shape = labels.get_shape().merge_with([batch_size, None])\n",
    "\n",
    "        features.set_shape(features_shape)\n",
    "        labels.set_shape(labels_shape)\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(set_shapes)\n",
    "    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # pass the args as params so the model_fn can use\n",
    "    # the TPU specific args\n",
    "    params = vars(args)\n",
    "\n",
    "    if args.use_tpu:\n",
    "        # additional configs required for using TPUs\n",
    "        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(args.tpu)\n",
    "        tpu_config = tf.contrib.tpu.TPUConfig(\n",
    "            num_shards=8, # using Cloud TPU v2-8\n",
    "            iterations_per_loop=args.save_checkpoints_steps\n",
    "        )\n",
    "\n",
    "        # use the TPU version of RunConfig\n",
    "        config = tf.contrib.tpu.RunConfig(\n",
    "            cluster=tpu_cluster_resolver,\n",
    "            model_dir=args.model_dir,\n",
    "            tpu_config=tpu_config,\n",
    "            save_checkpoints_steps=args.save_checkpoints_steps,\n",
    "            save_summary_steps=10\n",
    "        )\n",
    "\n",
    "        # TPUEstimator\n",
    "        estimator = tf.contrib.tpu.TPUEstimator(\n",
    "            model_fn=model_fn,\n",
    "            config=config,\n",
    "            params=params,\n",
    "            train_batch_size=args.train_batch_size,\n",
    "            eval_batch_size=32, # FIXME\n",
    "            export_to_tpu=False\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        config = tf.estimator.RunConfig(\n",
    "            model_dir=args.model_dir,\n",
    "            save_checkpoints_steps=10,\n",
    "            save_summary_steps=10\n",
    "        )\n",
    "\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn,\n",
    "            config=config,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "    estimator.train(train_input_fn, steps=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "    \n",
    "parser.add_argument(\n",
    "        '--model-dir',\n",
    "        type=str,\n",
    "        default='/tmp/tpu-template'\n",
    "    )\n",
    "parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        type=int,\n",
    "        default=16\n",
    "    )\n",
    "parser.add_argument(\n",
    "        '--save-checkpoints-steps',\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "parser.add_argument(\n",
    "        '--use-tpu',\n",
    "        action='store_true'\n",
    "    )\n",
    "parser.add_argument(\n",
    "        '--tpu',\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    \n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "    \n",
    "main(args)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
