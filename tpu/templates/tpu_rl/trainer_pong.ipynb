{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2018 Google LLC  \n",
    "  \n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    " you may not use this file except in compliance with the License.  \n",
    " You may obtain a copy of the License at  \n",
    "  \n",
    "     http://www.apache.org/licenses/LICENSE-2.0  \n",
    "  \n",
    " Unless required by applicable law or agreed to in writing, software  \n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    " See the License for the specific language governing permissions and  \n",
    " limitations under the License.  \n",
    " Reference: https://colab.research.google.com/gist/rjpower/169b2843a506d090f47d25122f82a28f  \n",
    " Based on: https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/tpu/templates/tpu_rewrite/trainer_infeed_outfeed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for when running on Colab:\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Get the dependency .py files, if any.\n",
    "    ! git clone https://github.com/GoogleCloudPlatform/cloudml-samples.git\n",
    "    ! cp cloudml-samples/tpu/templates/tpu_rl/* .\n",
    "\n",
    "    # Authenticate the user for better GCS access.\n",
    "    # Copy verification code into the text field to continue.\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import threading\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "from tf_agents.environments import suite_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tf_agents.environments import tf_py_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the first channel of state downsampled by a factor of 2 as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SIZE = 80 * 80\n",
    "ACTIONS = [0, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size of the experience gathered at each rollout phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLOUT_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of rollouts needed to fill up the experience cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROLLOUTS = 10\n",
    "EXPERIENCE_LENGTH = ROLLOUT_LENGTH * N_ROLLOUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper taken from: # https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(features):\n",
    "    with tf.variable_scope('agent', reuse=tf.AUTO_REUSE):\n",
    "        hidden = tf.layers.dense(features, 200, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden, len(ACTIONS))\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(features, actions, rewards):\n",
    "    # features are observations\n",
    "\n",
    "    logits = policy(features)\n",
    "    onehot_labels = tf.one_hot(actions, depth=len(ACTIONS))\n",
    "    loss = tf.reduce_sum(rewards * tf.nn.softmax_cross_entropy_with_logits_v2(labels=onehot_labels, logits=logits))\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    return global_step, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpu_computation_with_infeed(batch_size, num_shards):\n",
    "    # TPU device perspective.\n",
    "\n",
    "    features, actions, rewards = tf.contrib.tpu.infeed_dequeue_tuple(\n",
    "        # the dtypes and shapes need to be consistent with what is fed into the infeed queue.\n",
    "        dtypes=[tf.float32, tf.int32, tf.float32],\n",
    "        shapes=[\n",
    "            (batch_size // num_shards, FEATURE_SIZE),\n",
    "            (batch_size // num_shards, ),\n",
    "            (batch_size // num_shards, )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    global_step, loss, train_op = fit_batch(features, actions, rewards)\n",
    "\n",
    "    return tf.contrib.tpu.outfeed_enqueue_tuple((global_step, loss)), train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_setup_feed(features, actions, rewards, num_shards):\n",
    "    # CPU perspective.\n",
    "\n",
    "    infeed_ops = []\n",
    "    outfeed_ops = []\n",
    "\n",
    "    infeed_batches = zip(\n",
    "        tf.split(features, num_shards),\n",
    "        tf.split(actions, num_shards),\n",
    "        tf.split(rewards, num_shards)\n",
    "    )\n",
    "\n",
    "    for i, batch in enumerate(infeed_batches):\n",
    "        infeed_op = tf.contrib.tpu.infeed_enqueue_tuple(\n",
    "            batch,\n",
    "            [b.shape for b in batch],\n",
    "            device_ordinal=i\n",
    "        )\n",
    "        infeed_ops.append(infeed_op)\n",
    "\n",
    "        outfeed_op = tf.contrib.tpu.outfeed_dequeue_tuple(\n",
    "                dtypes=[tf.int64, tf.float32],\n",
    "                shapes=[(), ()],\n",
    "                device_ordinal=i\n",
    "            )\n",
    "        outfeed_ops.append(outfeed_op)\n",
    "\n",
    "    return infeed_ops, outfeed_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds(v, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(v)\n",
    "    dataset = dataset.repeat().shuffle(ROLLOUT_LENGTH).batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "\n",
    "    n_dim = len(next_batch.shape)\n",
    "    merge_shape = [batch_size] + [None] * (n_dim - 1)\n",
    "    shape = next_batch.shape.merge_with(merge_shape) \n",
    "    next_batch.set_shape(shape)\n",
    "\n",
    "    return next_batch, iterator.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_shape(tensor, batch_size):\n",
    "    n_dim = len(tensor.shape)\n",
    "    merge_shape = [batch_size] + [None] * (n_dim - 1)\n",
    "    shape = tensor.shape.merge_with(merge_shape) \n",
    "    tensor.set_shape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # use variables to store experience\n",
    "    features_var = tf.get_variable('features', dtype=tf.float32, shape=[EXPERIENCE_LENGTH, FEATURE_SIZE], trainable=False)\n",
    "    actions_var = tf.get_variable('actions', dtype=tf.int32, shape=[EXPERIENCE_LENGTH], trainable=False)\n",
    "    rewards_var = tf.get_variable('rewards', dtype=tf.float32, shape=[EXPERIENCE_LENGTH], trainable=False)\n",
    "\n",
    "    # wrap the experience variables in a dict to shuffle them together\n",
    "    experience = {'features': features_var, 'actions': actions_var, 'rewards': rewards_var}\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(experience)\n",
    "    dataset = dataset.repeat().shuffle(32).batch(args.train_batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "\n",
    "    for tensor in next_batch.values():\n",
    "        set_shape(tensor, args.train_batch_size)\n",
    "\n",
    "    features = next_batch['features']\n",
    "    actions = next_batch['actions']\n",
    "    rewards = next_batch['rewards']\n",
    "\n",
    "    ds_init = iterator.initializer\n",
    "\n",
    "    infeed_ops, outfeed_ops = cpu_setup_feed(features, actions, rewards, num_shards=8)\n",
    "\n",
    "    # Wrap the tpu computation function to be run in a loop.\n",
    "    def computation_loop():\n",
    "        return tf.contrib.tpu.repeat(args.iterations_per_loop, partial(tpu_computation_with_infeed, batch_size=args.train_batch_size, num_shards=8))\n",
    "\n",
    "    tpu_computation_loop = tf.contrib.tpu.batch_parallel(computation_loop, num_shards=8)\n",
    "\n",
    "    # CPU policy used for interacting with the environment\n",
    "    # Batch size of 1 for rollout against a single environment.\n",
    "    features_ph = tf.placeholder(dtype=tf.float32, shape=(1, FEATURE_SIZE)) \n",
    "    rollout_logits = policy(features_ph)\n",
    "    rollout_actions = tf.squeeze(tf.random.categorical(logits=rollout_logits, num_samples=1))\n",
    "\n",
    "    # placeholders and ops for updating after rollout\n",
    "    features_var_ph = tf.placeholder(dtype=features_var.dtype, shape=features_var.shape)\n",
    "    actions_var_ph = tf.placeholder(dtype=actions_var.dtype, shape=actions_var.shape)\n",
    "    rewards_var_ph = tf.placeholder(dtype=rewards_var.dtype, shape=rewards_var.shape)\n",
    "\n",
    "    update_features_op = tf.assign(features_var, features_var_ph)\n",
    "    update_actions_op = tf.assign(actions_var, actions_var_ph)\n",
    "    update_rewareds_op = tf.assign(rewards_var, rewards_var_ph)\n",
    "\n",
    "\n",
    "    # rollout_actions = tf.squeeze(tf.random.multinomial(logits=rollout_logits, num_samples=1))\n",
    "\n",
    "    # utility ops\n",
    "    tpu_init = tf.contrib.tpu.initialize_system()\n",
    "    tpu_shutdown = tf.contrib.tpu.shutdown_system()\n",
    "    variables_init = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # with tf.name_scope('summaries'):\n",
    "    #     summary_reward = tf.placeholder(\n",
    "    #         shape=(),\n",
    "    #         dtype=tf.float32\n",
    "    #     )\n",
    "\n",
    "    #     # the weights to the hidden layer can be visualized\n",
    "    #     hidden_weights = tf.trainable_variables()[0]\n",
    "    #     for h in range(200):\n",
    "    #         slice_ = tf.slice(hidden_weights, [0, h], [-1, 1])\n",
    "    #         image = tf.reshape(slice_, [1, 105, 80, 1])\n",
    "    #         tf.summary.image('hidden_{:04d}'.format(h), image)\n",
    "\n",
    "    #     for var in tf.trainable_variables():\n",
    "    #         tf.summary.histogram(var.op.name, var)\n",
    "    #         tf.summary.scalar('{}_max'.format(var.op.name), tf.reduce_max(var))\n",
    "    #         tf.summary.scalar('{}_min'.format(var.op.name), tf.reduce_min(var))\n",
    "            \n",
    "    #     tf.summary.scalar('rollout_reward', summary_reward)\n",
    "    #     # tf.summary.scalar('loss', loss)\n",
    "\n",
    "    #     merged = tf.summary.merge_all()\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(args.model_dir)\n",
    "    summary_writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "    # get the TPU resource's grpc url\n",
    "    # Note: when running on CMLE, args.tpu should be left as None\n",
    "    tpu_grpc_url = TPUClusterResolver(tpu=args.tpu).get_master()\n",
    "    sess = tf.Session(tpu_grpc_url)\n",
    "\n",
    "    # Use separate threads to run infeed and outfeed.\n",
    "    def _run_infeed():\n",
    "        for i in range(args.max_steps):\n",
    "            time.sleep(2)\n",
    "            sess.run(infeed_ops)\n",
    "\n",
    "            if i % args.save_checkpoints_steps == 0:\n",
    "                print('infeed {}'.format(i))\n",
    "\n",
    "    def _run_infeed1(input_queue):\n",
    "        thread = threading.currentThread()\n",
    "        while thread.do_work:\n",
    "            if input_queue.empty():\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                i = input_queue.get()\n",
    "\n",
    "                if i % args.save_checkpoints_steps == 0:\n",
    "                    print('infeed {}'.format(i))\n",
    "\n",
    "                for _ in range(args.iterations_per_loop):\n",
    "                    sess.run(infeed_ops)\n",
    "\n",
    "                input_queue.task_done()\n",
    "\n",
    "\n",
    "    def _run_outfeed():\n",
    "        for i in range(args.max_steps):\n",
    "            outfeed_data = sess.run(outfeed_ops)\n",
    "\n",
    "            if i % args.save_checkpoints_steps == 0:\n",
    "                print('outfeed {}'.format(i))\n",
    "                print('data returned from outfeed: {}'.format(outfeed_data))\n",
    "\n",
    "\n",
    "    def _run_tpu_computation(tpu_queue):\n",
    "        thread = threading.currentThread()\n",
    "        while thread.do_work:\n",
    "            if not tpu_queue.empty():\n",
    "                v = tpu_queue.get()\n",
    "                sess.run(tpu_computation_loop)\n",
    "                print('tpu computation: {}'.format(v))\n",
    "\n",
    "                tpu_queue.task_done()\n",
    "\n",
    "    # https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "    def state_to_features(state):\n",
    "        I = state[35:195]\n",
    "        I = I[::2, ::2, 0]\n",
    "        I[I == 144] = 0\n",
    "        I[I == 109] = 0\n",
    "        I[I != 0] = 1\n",
    "        return I.astype(float).reshape((1, FEATURE_SIZE))\n",
    "\n",
    "    def action_to_env_action(action):\n",
    "        if action in range(3):\n",
    "            return ACTIONS[action]\n",
    "        else:\n",
    "            return random.choice(ACTIONS)\n",
    "\n",
    "    # initialize env\n",
    "    env = suite_gym.load('Pong-v0')\n",
    "\n",
    "    # In the main thread, interact with th environment and collect data into the experience variables.\n",
    "    def run_rollout():\n",
    "        start_time = time.time()\n",
    "\n",
    "        ts = env.reset()\n",
    "        state = ts.observation\n",
    "        reward = ts.reward\n",
    "        done = ts.is_last()\n",
    "\n",
    "        step_features = state_to_features(state)\n",
    "\n",
    "        batch_features = []\n",
    "        batch_actions = []\n",
    "        batch_rewards = []\n",
    "\n",
    "        # for debugging\n",
    "        batch_logits = []\n",
    "\n",
    "        # collect data up to the point when a point is scored\n",
    "        while reward == 0 or len(batch_features) <= ROLLOUT_LENGTH:\n",
    "            # Since the CPU and the TPU share the model variables, this is using the updated policy.\n",
    "            # step_actions = sess.run(rollout_actions, {features_ph: step_features})\n",
    "\n",
    "            # for debugging\n",
    "            [step_actions, step_logits] = sess.run([rollout_actions, rollout_logits], {features_ph: step_features})\n",
    "\n",
    "            env_action = action_to_env_action(step_actions)\n",
    "\n",
    "            ts = env.step(env_action)\n",
    "            state = ts.observation\n",
    "            reward = ts.reward\n",
    "            done = ts.is_last()\n",
    "\n",
    "            batch_features.append(step_features)\n",
    "            batch_actions.append(step_actions)\n",
    "            batch_rewards.append(reward)\n",
    "\n",
    "            # for debugging\n",
    "            batch_logits.append(step_logits)\n",
    "\n",
    "            if done:\n",
    "                ts = env.reset()\n",
    "                state = ts.observation\n",
    "\n",
    "            step_features = state_to_features(state)\n",
    "\n",
    "        print('>>>>>>> collected {} steps, {}'.format(len(batch_features), time.time() - start_time))\n",
    "\n",
    "        # udpate experience variables\n",
    "        batch_features = np.array(batch_features).squeeze()\n",
    "        batch_actions = np.array(batch_actions)\n",
    "        batch_rewards = np.array(batch_rewards)\n",
    "\n",
    "        # for debugging:\n",
    "        # print(batch_actions)\n",
    "        # print(batch_logits)\n",
    "        sum_reward = batch_rewards.sum()\n",
    "        print('>>>>>>> {}'.format(sum_reward))\n",
    "\n",
    "        # summary, gs = sess.run([merged, tf.train.get_or_create_global_step()], feed_dict={summary_reward: sum_reward})\n",
    "        # summary_writer.add_summary(summary, gs)\n",
    "\n",
    "        # process the rewards\n",
    "        batch_rewards = discount_rewards(batch_rewards, 0.95)\n",
    "        batch_rewards -= np.mean(batch_rewards)\n",
    "        batch_rewards /= np.std(batch_rewards)\n",
    "\n",
    "        fv, av, rv = sess.run([features_var, actions_var, rewards_var])\n",
    "        new_fv = np.concatenate([fv[ROLLOUT_LENGTH:], batch_features[-ROLLOUT_LENGTH:]])\n",
    "        new_av = np.concatenate([av[ROLLOUT_LENGTH:], batch_actions[-ROLLOUT_LENGTH:]])\n",
    "        new_rv = np.concatenate([rv[ROLLOUT_LENGTH:], batch_rewards[-ROLLOUT_LENGTH:]])\n",
    "\n",
    "        sess.run([update_features_op, update_actions_op, update_rewareds_op], {features_var_ph: new_fv, actions_var_ph: new_av, rewards_var_ph: new_rv})\n",
    "\n",
    "    tpu_queue = Queue(maxsize=0)\n",
    "    input_queue = Queue(maxsize=0)\n",
    "\n",
    "    infeed_thread = threading.Thread(target=_run_infeed1, args=(input_queue,))\n",
    "    infeed_thread.do_work = True\n",
    "\n",
    "    outfeed_thread = threading.Thread(target=_run_outfeed)\n",
    "\n",
    "    tpu_thread = threading.Thread(target=_run_tpu_computation, args=(tpu_queue,))\n",
    "    tpu_thread.do_work = True\n",
    "\n",
    "    sess.run(tpu_init)\n",
    "    sess.run(variables_init)\n",
    "    sess.run(ds_init)\n",
    "\n",
    "    for _ in range(N_ROLLOUTS):\n",
    "        run_rollout()\n",
    "\n",
    "    input_queue.put(-1)\n",
    "\n",
    "    infeed_thread.start()\n",
    "    outfeed_thread.start()\n",
    "    tpu_thread.start()\n",
    "\n",
    "    for i in range(args.num_loops):\n",
    "        print('Iteration: {}'.format(i))\n",
    "\n",
    "        tpu_queue.put(i)\n",
    "        input_queue.put(i)\n",
    "        run_rollout()\n",
    "\n",
    "        gs = sess.run(tf.train.get_or_create_global_step())\n",
    "\n",
    "        if i % args.save_checkpoints_steps == 0:\n",
    "            saver.save(sess, os.path.join(args.model_dir, 'model.ckpt'), global_step=gs)\n",
    "\n",
    "    tpu_thread.do_work = False\n",
    "    infeed_thread.do_work = False\n",
    "\n",
    "    # input_queue.join()\n",
    "    # tpu_queue.join()\n",
    "    infeed_thread.join()\n",
    "    outfeed_thread.join()\n",
    "    tpu_thread.join()\n",
    "\n",
    "    sess.run(tpu_shutdown)\n",
    "\n",
    "    saver.save(sess, os.path.join(args.model_dir, 'model.ckpt'), global_step=args.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--model-dir',\n",
    "    type=str,\n",
    "    default='/tmp/tpu-template',\n",
    "    help='Location to write checkpoints and summaries to.  Must be a GCS URI when using Cloud TPU.')\n",
    "parser.add_argument(\n",
    "    '--iterations-per-loop',\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help='The number of iterations on TPU before switching to CPU.')\n",
    "parser.add_argument(\n",
    "    '--num-loops',\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help='The number of times switching to CPU.')\n",
    "parser.add_argument(\n",
    "    '--save-checkpoints-steps',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='The number of training steps before saving each checkpoint.')\n",
    "parser.add_argument(\n",
    "    '--train-batch-size',\n",
    "    type=int,\n",
    "    default=8192,\n",
    "    help='The training batch size.  The training batch is divided evenly across the TPU cores.')\n",
    "parser.add_argument(\n",
    "    '--tpu',\n",
    "    default=None,\n",
    "    help='The name or GRPC URL of the TPU node.  Leave it as `None` when training on CMLE.')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "args.max_steps = args.iterations_per_loop * args.num_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab.research.google.com specific\n",
    "if 'google.colab' in sys.modules:\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    # TODO(user): change this\n",
    "    args.model_dir = 'gs://your-gcs-bucket'\n",
    "\n",
    "    # When connected to the TPU runtime\n",
    "    if 'COLAB_TPU_ADDR' in os.environ:\n",
    "        tpu_grpc = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
    "\n",
    "        args.tpu = tpu_grpc\n",
    "        args.use_tpu = True\n",
    "\n",
    "        # Upload credentials to the TPU\n",
    "        with tf.Session(tpu_grpc) as sess:\n",
    "            data = json.load(open('/content/adc.json'))\n",
    "            tf.contrib.cloud.configure_gcs(sess, credentials=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
