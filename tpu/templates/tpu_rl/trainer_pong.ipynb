{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2018 Google LLC  \n",
    "  \n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    " you may not use this file except in compliance with the License.  \n",
    " You may obtain a copy of the License at  \n",
    "  \n",
    "     http://www.apache.org/licenses/LICENSE-2.0  \n",
    "  \n",
    " Unless required by applicable law or agreed to in writing, software  \n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    " See the License for the specific language governing permissions and  \n",
    " limitations under the License.  \n",
    " Reference: https://colab.research.google.com/gist/rjpower/169b2843a506d090f47d25122f82a28f  \n",
    " Based on: https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/tpu/templates/tpu_rewrite/trainer_infeed_outfeed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for when running on Colab:\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Get the dependency .py files, if any.\n",
    "    ! git clone https://github.com/GoogleCloudPlatform/cloudml-samples.git\n",
    "    ! cp cloudml-samples/tpu/templates/tpu_rl/* .\n",
    "\n",
    "    # Authenticate the user for better GCS access.\n",
    "    # Copy verification code into the text field to continue.\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import threading\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "from Queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the first channel of state downsampled by a factor of 2 as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SIZE = 105 * 80\n",
    "ACTION_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size of the experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLOUT_LENGTH = 128\n",
    "N_ROLLOUTS = 2\n",
    "EXPERIENCE_LENGTH = ROLLOUT_LENGTH * N_ROLLOUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(features):\n",
    "    with tf.variable_scope('agent', reuse=tf.AUTO_REUSE):\n",
    "        hidden = tf.layers.dense(features, 200, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden, ACTION_SIZE)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(features, actions, rewards):\n",
    "    # features are observations\n",
    "\n",
    "    logits = policy(features)\n",
    "    loss = rewards * tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions, logits=logits)\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.05)\n",
    "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    return global_step, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpu_computation_with_infeed(batch_size, num_shards):\n",
    "    # TPU device perspective.\n",
    "\n",
    "    features, actions, rewards = tf.contrib.tpu.infeed_dequeue_tuple(\n",
    "        # the dtypes and shapes need to be consistent with what is fed into the infeed queue.\n",
    "        dtypes=[tf.float32, tf.int32, tf.float32],\n",
    "        shapes=[\n",
    "            (batch_size // num_shards, FEATURE_SIZE),\n",
    "            (batch_size // num_shards, ACTION_SIZE),\n",
    "            (batch_size // num_shards)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    global_step, loss, train_op = fit_batch(features, actions, rewards)\n",
    "\n",
    "    return tf.contrib.tpu.outfeed_enqueue_tuple((global_step, loss)), train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_setup_feed(features, actions, rewards, num_shards):\n",
    "    # CPU perspective.\n",
    "\n",
    "    infeed_ops = []\n",
    "    outfeed_ops = []\n",
    "\n",
    "    infeed_batches = zip(\n",
    "        tf.split(features, num_shards),\n",
    "        tf.split(actions, num_shards),\n",
    "        tf.split(rewards, num_shards)\n",
    "    )\n",
    "\n",
    "    for i, batch in enumerate(infeed_batches):\n",
    "        infeed_op = tf.contrib.tpu.infeed_enqueue_tuple(\n",
    "            batch,\n",
    "            [b.shape for b in batch],\n",
    "            device_ordinal=i\n",
    "        )\n",
    "        infeed_ops.append(infeed_op)\n",
    "\n",
    "        outfeed_op = tf.contrib.tpu.outfeed_dequeue_tuple(\n",
    "                dtypes=[tf.int64, tf.float32],\n",
    "                shapes=[(), ()],\n",
    "                device_ordinal=i\n",
    "            )\n",
    "        outfeed_ops.append(outfeed_op)\n",
    "\n",
    "    return infeed_ops, outfeed_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds(v, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(v)\n",
    "    dataset = dataset.repeat().shuffle(32).batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "\n",
    "    n_dim = len(next_batch.shape)\n",
    "    merge_shape = [batch_size] + [None] * (n_dim - 1)\n",
    "    shape = next_batch.shape.merge_with(merge_shape) \n",
    "    next_batch.set_shape(shape)\n",
    "\n",
    "    return next_batch, iterator.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_deque(name, dtype, shape, update_size):\n",
    "    variable = tf.get_variable(name, dtype=dtype, shape=shape, trainable=False)\n",
    "\n",
    "    update_shape = [update_size] + shape[1:]\n",
    "    update_ph = tf.placeholder(dtype=dtype, shape=update_shape)\n",
    "\n",
    "    updated_value = tf.concat([variable[update_size:], update_ph], axis=0)\n",
    "\n",
    "    update_op = variable.assign(updated_value)\n",
    "\n",
    "    return variable, update_ph, update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # use variables to store experience\n",
    "    features, update_features_ph, update_features_op = tf_deque('features', tf.float32, [EXPERIENCE_LENGTH, FEATURE_SIZE], ROLLOUT_LENGTH)\n",
    "    actions, update_actions_ph, update_actions_op = tf_deque('actions', tf.int32, [EXPERIENCE_LENGTH, ACTION_SIZE], ROLLOUT_LENGTH)\n",
    "    rewards, update_rewards_ph, update_rewards_op = tf_deque('rewards', tf.float32, [EXPERIENCE_LENGTH,], ROLLOUT_LENGTH)\n",
    "\n",
    "    rollout_update_ops = [update_features_op, update_actions_op, update_rewards_op]\n",
    "\n",
    "    features, features_init = make_ds(features, args.train_batch_size)\n",
    "    actions, actions_init = make_ds(actions, args.train_batch_size)\n",
    "    rewards, rewards_init = make_ds(rewards, args.train_batch_size)\n",
    "\n",
    "    ds_inits = [features_init, actions_init, rewards_init]\n",
    "\n",
    "    infeed_ops, outfeed_ops = cpu_setup_feed(features, actions, rewards, num_shards=8)\n",
    "\n",
    "    # Wrap the tpu computation function to be run in a loop.\n",
    "    def computation_loop():\n",
    "        return tf.contrib.tpu.repeat(args.iterations_per_loop, partial(tpu_computation_with_infeed, batch_size=128, num_shards=8))\n",
    "\n",
    "    tpu_computation_loop = tf.contrib.tpu.batch_parallel(computation_loop, num_shards=8)\n",
    "\n",
    "    # CPU policy used for interacting with the environment\n",
    "    # Batch size of 1 for rollout against a single environment.\n",
    "    features_ph = tf.placeholder(dtype=tf.float32, shape=(1, FEATURE_SIZE))\n",
    "    rollout_logits = policy(features_ph)\n",
    "    rollout_actions = tf.squeeze(tf.multinomial(logits=rollout_logits, num_samples=1))\n",
    "\n",
    "    # utility ops\n",
    "    tpu_init = tf.contrib.tpu.initialize_system()\n",
    "    tpu_shutdown = tf.contrib.tpu.shutdown_system()\n",
    "    variables_init = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # get the TPU resource's grpc url\n",
    "    # Note: when running on CMLE, args.tpu should be left as None\n",
    "    tpu_grpc_url = TPUClusterResolver(tpu=args.tpu).get_master()\n",
    "    sess = tf.Session(tpu_grpc_url)\n",
    "\n",
    "    # Use separate threads to run infeed and outfeed.\n",
    "    def _run_infeed(rollout_q):\n",
    "        while True:\n",
    "            if not rollout_q.empty():\n",
    "                print('infeeding data after rollout {}'.format(q.get()))\n",
    "\n",
    "                for i in range(args.iterations_per_loop):\n",
    "                    sess.run(infeed_ops)\n",
    "\n",
    "                    if i % args.save_checkpoints_steps == 0:\n",
    "                        print('infeed {}'.format(i))\n",
    "\n",
    "                rollout_q.task_done()\n",
    "\n",
    "\n",
    "    def _run_outfeed():\n",
    "        for i in range(args.iterations_per_loop):\n",
    "            outfeed_data = sess.run(outfeed_ops)\n",
    "\n",
    "            if i % args.save_checkpoints_steps == 0:\n",
    "                print('outfeed {}'.format(i))\n",
    "                print('data returned from outfeed: {}'.format(outfeed_data))\n",
    "\n",
    "                saver.save(sess, os.path.join(args.model_dir, 'model.ckpt'), global_step=i)\n",
    "\n",
    "\n",
    "    def state_to_features(state):\n",
    "        return state[::2, ::2, 0].reshape((1, FEATURE_SIZE))\n",
    "\n",
    "    def action_to_env_action(action):\n",
    "        ACTIONS = [0, 2, 3]\n",
    "        return ACTIONS[action]\n",
    "\n",
    "    # initialize env\n",
    "    import gym\n",
    "    env = gym.make('Pong-v0')\n",
    "\n",
    "    # In the main thread, interact with th environment and collect data into the experience variables.\n",
    "    def run_rollout():\n",
    "        print('start rollout: {}'.format(datetime.datetime.now()))\n",
    "        state = env.reset()\n",
    "        step_features = state_to_features(state)\n",
    "\n",
    "        batch_features = []\n",
    "        batch_actions = []\n",
    "        batch_rewards = []\n",
    "\n",
    "        while len(batch_features) < ROLLOUT_LENGTH:\n",
    "            # Since the CPU and the TPU share the model variables, this is using the updated policy.\n",
    "            step_actions = sess.run(rollout_actions, {features_ph: step_features})\n",
    "\n",
    "            env_action = action_to_env_action(step_actions)\n",
    "            onehot_action = sess.run(tf.one_hot(step_actions, depth=3))\n",
    "\n",
    "            state, reward, done, _ = env.step(env_action)\n",
    "\n",
    "            batch_features.append(step_features.tolist())\n",
    "            batch_actions.append(onehot_action)\n",
    "            batch_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "            step_features = state_to_features(state)\n",
    "\n",
    "        print('end rollout: {}'.format(datetime.datetime.now()))\n",
    "\n",
    "        rollout_feed_dict = {\n",
    "            update_features_ph: np.array(batch_features).squeeze(),\n",
    "            update_actions_ph: np.array(batch_actions),\n",
    "            update_rewards_ph: np.array(batch_rewards)\n",
    "        }\n",
    "        sess.run(rollout_update_ops, rollout_feed_dict)\n",
    "\n",
    "    # 0 means unlimited size\n",
    "    rollout_q = Queue(maxsize=0)\n",
    "\n",
    "    infeed_thread = threading.Thread(target=_run_infeed, args=(rollout_q,))\n",
    "    outfeed_thread = threading.Thread(target=_run_outfeed)\n",
    "\n",
    "    sess.run(tpu_init)\n",
    "    sess.run(variables_init)\n",
    "    sess.run(ds_inits)\n",
    "\n",
    "    for _ in range(N_ROLLOUTS):\n",
    "        run_rollout()\n",
    "\n",
    "    infeed_thread.start()\n",
    "    outfeed_thread.start()\n",
    "\n",
    "    for i in range(args.num_loops):\n",
    "        print('Iteration: {}'.format(i))\n",
    "\n",
    "        sess.run(tpu_computation_loop)\n",
    "\n",
    "        run_rollout()\n",
    "        rollout_q.put(i)\n",
    "        \n",
    "\n",
    "    # infeed_thread.join()\n",
    "    rollout_q.join()\n",
    "    outfeed_thread.join()\n",
    "\n",
    "    sess.run(tpu_shutdown)\n",
    "\n",
    "    saver.save(sess, os.path.join(args.model_dir, 'model.ckpt'), global_step=args.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--model-dir',\n",
    "    type=str,\n",
    "    default='/tmp/tpu-template',\n",
    "    help='Location to write checkpoints and summaries to.  Must be a GCS URI when using Cloud TPU.')\n",
    "parser.add_argument(\n",
    "    '--iterations-per-loop',\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help='The number of iterations on TPU before switching to CPU.')\n",
    "parser.add_argument(\n",
    "    '--num-loops',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='The number of times switching to CPU.')\n",
    "parser.add_argument(\n",
    "    '--save-checkpoints-steps',\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help='The number of training steps before saving each checkpoint.')\n",
    "parser.add_argument(\n",
    "    '--train-batch-size',\n",
    "    type=int,\n",
    "    default=512,\n",
    "    help='The training batch size.  The training batch is divided evenly across the TPU cores.')\n",
    "parser.add_argument(\n",
    "    '--tpu',\n",
    "    default=None,\n",
    "    help='The name or GRPC URL of the TPU node.  Leave it as `None` when training on CMLE.')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "args.max_steps = args.iterations_per_loop * args.num_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab.research.google.com specific\n",
    "if 'google.colab' in sys.modules:\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    # TODO(user): change this\n",
    "    args.model_dir = 'gs://your-gcs-bucket'\n",
    "\n",
    "    # When connected to the TPU runtime\n",
    "    if 'COLAB_TPU_ADDR' in os.environ:\n",
    "        tpu_grpc = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
    "\n",
    "        args.tpu = tpu_grpc\n",
    "        args.use_tpu = True\n",
    "\n",
    "        # Upload credentials to the TPU\n",
    "        with tf.Session(tpu_grpc) as sess:\n",
    "            data = json.load(open('/content/adc.json'))\n",
    "            tf.contrib.cloud.configure_gcs(sess, credentials=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
